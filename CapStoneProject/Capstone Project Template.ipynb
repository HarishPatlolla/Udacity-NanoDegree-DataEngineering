{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "### Executive Summary\n",
    "\n",
    "The Business Objective of this project is to support the `analytical datawarehouse in Amazon RedShift( cloud data warehouse)` for `U.S. Customs and Border Protection`. It is meant to process the raw data from the source, transform and load to the datalakes to provide analytics ready data about visitors entering U.S. The analytics team at `U.S. Customs and Border Protection` can build the reports; which can help  for better resource allocation;thereby enhancing the visitors experience hassle-free on their entry into United States.  \n",
    "\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import os, re\n",
    "import configparser\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, when, lower, isnull, year, month, dayofmonth, hour, weekofyear, dayofweek, date_format,to_date\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_type(df, dict_obj):\n",
    "    \"\"\"Converts the column to the desired type\n",
    "     Arguments:   df : Spark dataframe \n",
    "                  dict_obj : Dictionary object\n",
    "                                  ( Key= column name ; \n",
    "                                    value = Type to be converted to)     \n",
    "    \"\"\"\n",
    "\n",
    "    #k=Key (Column name) ; #v=Value (Type)\n",
    "    for k,v in dict_obj.items():\n",
    "        if k in df.columns:\n",
    "            df = df.withColumn(k, df[k].cast(v))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def rename_columns(df, dict_map):\n",
    "    '''Rename the columns of the dataset\n",
    "      Arguments: df: Spark dataframe to be processed.\n",
    "                     dict_map: key=old_name value= new_name\n",
    "    '''\n",
    "    df = df.select([col(c).alias(dict_map.get(c, c)) for c in df.columns])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def convert_sas_date(df, cols):\n",
    "    \"\"\"\n",
    "    Convert dates in the SAS datatype to a date in a string format YYYY-MM-DD \n",
    "    Args:\n",
    "        df   : Spark Dataframe\n",
    "        cols : List of columns in the SAS date format to be convert\n",
    "    \"\"\"\n",
    "    \n",
    "    #user defined function \n",
    "    convert_sas_udf = udf(lambda x: x if x is None \n",
    "                                      else (timedelta(days=x) + datetime(1960, 1, 1)) .date().strftime(date_format))\n",
    "   \n",
    "    \n",
    "    for c in [c for c in cols if c in df.columns]:\n",
    "        df = df.withColumn(c, convert_sas_udf(df[c]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def date_diff(date1, date2):\n",
    "    '''\n",
    "    Calculates the difference in days between two dates\n",
    "    '''\n",
    "    if date2 is None:\n",
    "        return None\n",
    "    else:\n",
    "        a = datetime.strptime(date1, date_format)\n",
    "        b = datetime.strptime(date2, date_format)\n",
    "        delta = b - a\n",
    "        return delta.days\n",
    "\n",
    "\n",
    "def change_field_value_condition(df, change_list):\n",
    "    '''\n",
    "    Helper function used to rename column values based on condition.\n",
    "    \n",
    "    Args:\n",
    "        df (:obj:`SparkDataFrame`): Spark dataframe to be processed.\n",
    "        change_list (:obj: `list`): List of tuples in the format (field, old value, new value)\n",
    "    '''\n",
    "    for field, old, new in change_list:\n",
    "        df = df.withColumn(field, when(df[field] == old, new).otherwise(df[field]))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# User defined functions using Spark udf wrapper function to convert SAS dates into string dates in the format YYYY-MM-DD, to capitalize the first letters of the string and to calculate the difference between two dates in days.\n",
    "capitalize_udf = udf(lambda x: x if x is None else x.title())\n",
    "date_diff_udf = udf(date_diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "The complete solution is cloud based on top of `Amazon Web Services (AWS)`. \n",
    "\n",
    "In this project, the datasets are loaded into EMR Cluster, preprocessed with Apache Spark and stored back as Fact and Dimension Tables in AWS S3 bucket in parquet format. The DataQuality checks are performed on the processed data. The Analytics team can pull data into `OLAP datawarehouse` or `BI apps` for their convenient analysis.\n",
    "\n",
    "The main objective of this project is to develop a ETL pipeline to process huge amounts of source data using `Apache Spark`, store the staging data back to `Amazon S3` bucket (To allow multiple users across the continum to access concurrently), \n",
    "\n",
    "Technologies Used: `Python` `Apache Spark` `Amazon S3` \n",
    "\n",
    "![](architecture.png)\n",
    "\n",
    "https://github.com/fpcarneiro/data-engineer-project/blob/master/Capstone%20Project%20Template.ipynb\n",
    "\n",
    "\n",
    "### Data Source\n",
    "\n",
    "`I94 Immigration Data`: \n",
    "This data comes from the US National Tourism and Trade Office Source. This data records immigration records partitioned by month of every year.\n",
    "\n",
    "`World temperature Data`\n",
    "This dataset comes from Kaggle Source. Includes temperature recordings of cities around the world for a period of time\n",
    "\n",
    "\n",
    "`US City Demographic Data`\n",
    "This dataset comes from OpenSoft Source. Includes population formation of US states, like race and gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#creating a spark session\n",
    "spark=SparkSession.builder.master(\"local\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# using SQLContext to read parquet file\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading parquet file\n",
    "immigration = sqlContext.read.parquet('data/part-00013-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet')\n",
    "immigration.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235125"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ` Checking Null Values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #### The total number of rows are #### \n",
      "\n",
      "235125\n",
      "\n",
      " #### The count of Missing values by column are #### \n",
      "\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+------+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost| occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+------+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    130|  53704|  31818|   783|      0|    0|       0|  158785|234200|    130|  27807| 235096|  27807|    783|    311|  4428|122404|  82585|     0|18486|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+------+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n #### The total number of rows are #### \\n')\n",
    "print(immigration.count())\n",
    "\n",
    "#checking null values\n",
    "print('\\n #### The count of Missing values by column are #### \\n')\n",
    "immigration.select([count(when(isnull(c), c)).alias(c) for c in immigration.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Dropping unnecessary columns`\n",
    "    \n",
    "    1. Irrelevant columns\n",
    "    2. Columns with more than 60% of the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Number of columns in the Immigration data Before Dropping\n",
      "28\n",
      "#### Number of columns in the Immigration data After Dropping\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print('#### Number of columns in the Immigration data Before Dropping')\n",
    "print(len(immigration.columns))\n",
    "\n",
    "\n",
    "print('#### Number of columns in the Immigration data After Dropping')\n",
    "immigration=immigration.drop(\"count\", \"entdepd\", \"matflag\", \"entdepu\",\"dtaddto\", \"biryear\", \"admnum\",\"insnum\",\"entdepa\",\"occup\",\"visapost\")\n",
    "print(len(immigration.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Type Casting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "None\n",
      "############## After Type Casting \n",
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- dtadfile: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(immigration.printSchema())\n",
    "\n",
    "#Numeric columns\n",
    "integer_cols = ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'arrdate', 'i94mode', 'i94bir', 'i94visa', 'count', \n",
    "                    'biryear', 'dtadfile', 'depdate']\n",
    "    \n",
    "#Date Columns\n",
    "date_cols = ['arrdate', 'depdate']\n",
    "    \n",
    "# Type Casting columns to NUMERIC\n",
    "immigration = cast_type(immigration, dict(zip(integer_cols, len(integer_cols)*[IntegerType()])))\n",
    "    \n",
    "# Convert SAS date to a format of YYYY-MM-DD\n",
    "immigration = convert_sas_date(immigration, date_cols)\n",
    "\n",
    "print('############## After Type Casting ')\n",
    "print(immigration.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Feature Engineering of Stay Column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new columns to store the length of the visitor stay in the US\n",
    "#Stay= Departure Date - Arrival Date\n",
    "immigration = immigration.withColumn('stay', date_diff_udf(immigration.arrdate, immigration.depdate))\n",
    "    \n",
    "#Converting the derived column to integer\n",
    "immigration = cast_type(immigration, {'stay': IntegerType()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Date Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Taking distinct arrival dates\n",
    "arrdate = immigration.select('arrdate').distinct()\n",
    "\n",
    "#Taking distinct departure dates\n",
    "depdate = immigration.select('depdate').distinct()\n",
    "\n",
    "#Union \n",
    "dates = arrdate.union(depdate)\n",
    "\n",
    "dates = dates.withColumn(\"year\", year(dates.arrdate))\n",
    "dates = dates.withColumn(\"month\", month(dates.arrdate))\n",
    "dates = dates.withColumn(\"day\", dayofmonth(dates.arrdate))\n",
    "dates = dates.withColumn(\"weekofyear\", weekofyear(dates.arrdate))\n",
    "dates = dates.withColumn(\"dayofweek\", dayofweek(dates.arrdate))\n",
    "dates = dates.drop(\"date\").withColumnRenamed('arrdate', 'date')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Temperature Data\n",
    "\n",
    "    In the main immigration dataset, there is a country code for every country.\n",
    "    We are extracting country wise temperature; joining with a lookup with country name to extract the country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_temperature = spark.read.csv('GlobalLandTemperaturesByCity.csv',header=True)\n",
    "global_temperature.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `Aggregating data by Country`\n",
    "    There are multiple records for the same city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = global_temperature.groupby([\"Country\"]).agg({\"AverageTemperature\": \"avg\", \n",
    "                                 \"Latitude\": \"first\",\n",
    "                                 \"Longitude\": \"first\"})\\\n",
    "    .withColumnRenamed('avg(AverageTemperature)', 'Temperature')\\\n",
    "    .withColumnRenamed('first(Latitude)', 'Latitude')\\\n",
    "    .withColumnRenamed('first(Longitude)', 'Longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+--------+---------+-------------+\n",
      "| Country|       Temperature|Latitude|Longitude|Country_Lower|\n",
      "+--------+------------------+--------+---------+-------------+\n",
      "|    Chad|27.189829394812683|   8.84N|   15.41E|         chad|\n",
      "|Paraguay|22.784014312977117|  24.92S|   58.52W|     paraguay|\n",
      "|  Russia|  3.34726798287354|  53.84N|   91.36E|       russia|\n",
      "|   Yemen| 25.76840766445382|  13.66N|   45.41E|        yemen|\n",
      "| Senegal| 25.98417669449083|  15.27N|   17.50W|      senegal|\n",
      "+--------+------------------+--------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename countries to match the lookup datasets\n",
    "change_countries = [(\"Country\", \"Congo (Democratic Republic Of The)\", \"Congo\"), \n",
    "                        (\"Country\", \"Côte D'Ivoire\", \"Ivory Coast\")]\n",
    "countries = change_field_value_condition(countries, change_countries)\n",
    "countries = countries.withColumn('Country_Lower', lower(countries.Country))\n",
    "\n",
    "countries.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `loading the lookup table to extract country code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Code: string (nullable = true)\n",
      " |-- I94CTRY: string (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-----------+\n",
      "|Code|    I94CTRY|\n",
      "+----+-----------+\n",
      "| 582|     MEXICO|\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "| 316|    ALGERIA|\n",
      "| 102|    ANDORRA|\n",
      "+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.read.csv(\"LookUpTables/I94CIT_I94RES.csv\",header=True)\n",
    "print(res.printSchema())\n",
    "res.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------+\n",
      "|Code|    I94CTRY|Country_Lower|\n",
      "+----+-----------+-------------+\n",
      "| 582|     MEXICO|       mexico|\n",
      "| 236|AFGHANISTAN|  afghanistan|\n",
      "+----+-----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#converting code column to integer    \n",
    "res = cast_type(res, {\"Code\": IntegerType()})\n",
    "#creating a new column as lower case of I94CTRY\n",
    "res = res.withColumn('Country_Lower', lower(res.I94CTRY))\n",
    "res.show(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "change_res = [(\"I94CTRY\", \"BOSNIA-HERZEGOVINA\", \"BOSNIA AND HERZEGOVINA\"), \n",
    "                  (\"I94CTRY\", \"INVALID: CANADA\", \"CANADA\"),\n",
    "                  (\"I94CTRY\", \"CHINA, PRC\", \"CHINA\"),\n",
    "                  (\"I94CTRY\", \"GUINEA-BISSAU\", \"GUINEA BISSAU\"),\n",
    "                  (\"I94CTRY\", \"INVALID: PUERTO RICO\", \"PUERTO RICO\"),\n",
    "                  (\"I94CTRY\", \"INVALID: UNITED STATES\", \"UNITED STATES\")]\n",
    "\n",
    "res = change_field_value_condition(res, change_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Joining the countries dataset with lookup table to create the country dimmension table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------------+--------+---------+\n",
      "|Code|  Country|       Temperature|Latitude|Longitude|\n",
      "+----+---------+------------------+--------+---------+\n",
      "| 532|    Aruba|              null|    null|     null|\n",
      "| 110|  Finland| 3.711644535691719|  60.27N|   25.95E|\n",
      "| 438|Australia| 16.70146214247643|  34.56S|  138.16E|\n",
      "| 113|   Greece|16.347482714233163|  37.78N|   24.41E|\n",
      "| 126| Portugal|14.749674965924589|  39.38N|    8.32W|\n",
      "+----+---------+------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = res.join(countries, res.Country_Lower == countries.Country_Lower, how=\"left\")\n",
    "res = res.withColumn(\"Country\", when(isnull(res[\"Country\"]), capitalize_udf(res.I94CTRY)).otherwise(res[\"Country\"]))   \n",
    "res = res.drop(\"I94CTRY\", \"Country_Lower\")\n",
    "res.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U.S. City Demographic Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n",
      "None\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_cities_demographics = spark.read.csv(\"LookUpTables/us-cities-demographics.csv\",header=True,sep=';')\n",
    "print(us_cities_demographics.printSchema())\n",
    "us_cities_demographics.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Type Casting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_cols=['Count', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born']\n",
    "float_cols=['Median Age', 'Average Household Size']\n",
    "\n",
    "us_cities_demographics = cast_type(us_cities_demographics, dict(zip(integer_cols, len(integer_cols)*[IntegerType()])))\n",
    "us_cities_demographics = cast_type(us_cities_demographics, dict(zip(float_cols, len(float_cols)*[DoubleType()])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Aggregating the data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### \n",
      " Aggregating the data \n",
      "\n",
      "+------------+-----------+----------+-----------------------+------------------------+-----------------+-------------------------+-------------------+----------------------+-----------------------------+\n",
      "|        City|      State|State Code|first(Total Population)|first(Female Population)|first(Median Age)|first(Number of Veterans)|first(Foreign-born)|first(Male Population)|first(Average Household Size)|\n",
      "+------------+-----------+----------+-----------------------+------------------------+-----------------+-------------------------+-------------------+----------------------+-----------------------------+\n",
      "|   Rockville|   Maryland|        MD|                  66998|                   35793|             38.1|                     1990|              25047|                 31205|                          2.6|\n",
      "|Delray Beach|    Florida|        FL|                  66261|                   34042|             47.9|                     4232|              16639|                 32219|                         2.35|\n",
      "| Jersey City| New Jersey|        NJ|                 264277|                  132512|             34.3|                     4374|             109186|                131765|                         2.57|\n",
      "|    Gulfport|Mississippi|        MS|                  71872|                   38764|             35.1|                     6646|               3072|                 33108|                         2.54|\n",
      "|  Cincinnati|       Ohio|        OH|                 298537|                  154883|             32.7|                    13699|              16896|                143654|                         2.08|\n",
      "+------------+-----------+----------+-----------------------+------------------------+-----------------+-------------------------+-------------------+----------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('#################### \\n Aggregating the data \\n')\n",
    "\n",
    "#Aggregating the data by CITY and STATE\n",
    "aggregate_df = us_cities_demographics.groupBy([\"City\", \"State\", \"State Code\"]).agg({\"Median Age\": \"first\", \n",
    " \"Male Population\": \"first\",\n",
    " \"Female Population\": \"first\", \n",
    " \"Total Population\": \"first\", \n",
    " \"Number of Veterans\": \"first\",\n",
    " \"Foreign-born\": \"first\", \n",
    " \"Average Household Size\": \"first\"})\n",
    "\n",
    "aggregate_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `pivoting the columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|        City|     State|State Code|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n",
      "+------------+----------+----------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|Delray Beach|   Florida|        FL|                             null| 1696|                    21138|              6397| 40980|\n",
      "|   Rockville|  Maryland|        MD|                              594|17370|                     7533|              9197| 41692|\n",
      "| Jersey City|New Jersey|        NJ|                             3356|67610|                    65051|             79718| 99300|\n",
      "|    Alhambra|California|        CA|                              687|44067|                     1905|             31386| 20811|\n",
      "|  Cincinnati|      Ohio|        OH|                             3362| 7633|                   133430|              9121|162245|\n",
      "+------------+----------+----------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot Table to transform values of the column Race to different columns\n",
    "pivoted_df = us_cities_demographics.groupby([\"City\", \"State\", \"State Code\"]).pivot(\"Race\").sum(\"Count\")\n",
    "pivoted_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Joining aggregated and pivoted data to extract summary stats as well as stats for each race`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+---------------+----------------+---------+--------------+-----------+--------------+--------------------+-----------------------------+-----+----------------------+----------------+-----+\n",
      "|        City|   State|State Code|TotalPopulation|FemalePopulation|MedianAge|NumberVeterans|ForeignBorn|MalePopulation|AverageHouseholdSize|AmericanIndianAndAlaskaNative|Asian|BlackOrAfricanAmerican|HispanicOrLatino|White|\n",
      "+------------+--------+----------+---------------+----------------+---------+--------------+-----------+--------------+--------------------+-----------------------------+-----+----------------------+----------------+-----+\n",
      "|Delray Beach| Florida|        FL|          66261|           34042|     47.9|          4232|      16639|         32219|                2.35|                            0| 1696|                 21138|            6397|40980|\n",
      "|   Rockville|Maryland|        MD|          66998|           35793|     38.1|          1990|      25047|         31205|                 2.6|                          594|17370|                  7533|            9197|41692|\n",
      "+------------+--------+----------+---------------+----------------+---------+--------------+-----------+--------------+--------------------+-----------------------------+-----+----------------------+----------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Rename column names\n",
    "# Join the aggregated Df To Pivoted DF\n",
    "demographics = aggregate_df.join(other=pivoted_df, on=[\"City\", \"State\", \"State Code\"], how=\"inner\")\\\n",
    ".withColumnRenamed('first(Total Population)', 'TotalPopulation')\\\n",
    ".withColumnRenamed('first(Female Population)', 'FemalePopulation')\\\n",
    ".withColumnRenamed('first(Male Population)', 'MalePopulation')\\\n",
    ".withColumnRenamed('first(Median Age)', 'MedianAge')\\\n",
    ".withColumnRenamed('first(Number of Veterans)', 'NumberVeterans')\\\n",
    ".withColumnRenamed('first(Foreign-born)', 'ForeignBorn')\\\n",
    ".withColumnRenamed('first(Average Household Size)', 'AverageHouseholdSize')\\\n",
    ".withColumnRenamed('Hispanic or Latino', 'HispanicOrLatino')\\\n",
    ".withColumnRenamed('Black or African-American', 'BlackOrAfricanAmerican')\\\n",
    ".withColumnRenamed('American Indian and Alaska Native', 'AmericanIndianAndAlaskaNative')\n",
    "\n",
    "\n",
    "numeric_cols = ['TotalPopulation', 'FemalePopulation', 'MedianAge', 'NumberVeterans', 'ForeignBorn', 'MalePopulation', \n",
    "'AverageHouseholdSize','AmericanIndianAndAlaskaNative', 'Asian', 'BlackOrAfricanAmerican', \n",
    "'HispanicOrLatino', 'White']\n",
    "\n",
    "# Fill the null values with 0\n",
    "demographics = demographics.fillna(0, numeric_cols)\n",
    "\n",
    "print(demographics.show(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "STAR SCHEMA has been chosen to maintain the data integrity\n",
    "\n",
    "After acessing the data from different datasets, we have `Immigration`,`country`,`Demographics` tables \n",
    "\n",
    "1. `Immigration Table` - It forms the center of the data warehouse. It is the Fact table consisting information about the \n",
    "    visitors information like Arrival Date, Departure Date, Gender, AirLine, Type of Visa\n",
    "    *Primary Key = cicid ( Unique identifer for the Visitor )*\n",
    "    \n",
    "    \n",
    "2.  ` Date Dimension Table` - From the Immigration table, a date dimension table was created using the Unique \n",
    "      arrival and departure dates\n",
    "      *Primary Key = date*\n",
    "      \n",
    "        \n",
    "3.  `The STATE dimension table` contains aggregation of the demographics dataset by the State column. It contains the \n",
    "      overall statistics ( Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born)\n",
    "      and the same statistics for each of the race ((BlackOrAfricanAmerican, White, ForeignBorn, AmericanIndianAndAlaskaNative, \n",
    "      HispanicOrLatino, Asian)\n",
    "      *Primary Key State*\n",
    "      \n",
    "      \n",
    "4. `The COUNTRY dimention` completes our star schema model. It has the average temperature , lattitude , longitude \n",
    "    for each country\n",
    "    *Primary key COuntry*\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The `ETL` pipeline to `Extract` the data from the repository, `Transform` using Spark and `Load` into S3 buckets in parquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "For exploring the data, jupyter notebook has been used. The actual pipeline was built using python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run etl_process_using_spark.py inside etl_scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "`Immigration Table `\n",
    "\n",
    "|ColumnName|Description\n",
    "|-----|-------|\n",
    "CICID|Primary Key\n",
    "I94YR|Year\n",
    "I94MON|Month\n",
    "I94CIT|3 digit for the country code where the visitor was born. This is a FK to the COUNTRY dimension table\n",
    "I94RES|3 digit for the country code where the visitor resides in. This is a FK to the COUNTRY dimension table\n",
    "ARRDATE|Arrival date in the USA. This is a FK to the DATE dimension table\n",
    "I94MODE|Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "I94ADDR|State of arrival. This is a FK to the STATE dimension table\n",
    "DEPDATE|Departure date from the USA. This is a FK to the DATE dimension table\n",
    "I94BIR|Age of Respondent in Years\n",
    "I94VISA|Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student)\n",
    "BIRYEAR|4 digit year of birth\n",
    "GENDER|Gender\n",
    "AIRLINE|Airline used to arrive in U.S.\n",
    "FLTNO|Flight number of Airline used to arrive in U.S.\n",
    "VISATYPE|Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "STAY|Number of days in the US\n",
    "\n",
    "\n",
    "`DATE TABLE`\n",
    "\n",
    "|ColumnName|Description\n",
    "|-----|-------|\n",
    "date|Date in the format YYYY-MM-DD. This is the PK.\n",
    "day|Two digit day\n",
    "month|Two digit month\n",
    "year|Four digit for the year\n",
    "weekofyear|The week of the year\n",
    "dayofweek|The day of the week\n",
    "\n",
    "`COUNTRY TABLE`\n",
    "\n",
    "|ColumnName|Description\n",
    "|-----|-------|\n",
    "Code|Country Code. This is the PK.\n",
    "Country|Country Name\n",
    "Temperature|Average temperature of the country between 1743 and 2013\n",
    "Latitude|GPS Latitude\n",
    "Longitude|GPS Longitude\n",
    "\n",
    "\n",
    "`STATE TABLE`\n",
    "\n",
    "|ColumnName|Description\n",
    "|-----|-------|\n",
    "Code|Primary Key. This is the code of the State as in I94ADDR lookup table\n",
    "State|Name of the state\n",
    "BlackOrAfricanAmerican|Number of residents of the race Black Or African American\n",
    "White|Number of residents of the race White\n",
    "ForeignBorn|Number of residents that born outside th United States\n",
    "AmericanIndianAndAlaskaNative|Number of residents of the race American Indian And Alaska Native\n",
    "HispanicOrLatino|Number of residents of the race Hispanic Or Latino\n",
    "Asian|Number of residents of the race Asian\n",
    "NumberVeterans|Number of residents that are war veterans\n",
    "FemalePopulation|Number of female population\n",
    "MalePopulation|Number of male population\n",
    "TotalPopulation|Number total of the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "#### `Clearly state the rationale for the choice of tools and technologies for the project`\n",
    "\n",
    "The whole solution was implemented on cloud Amazon Web Services because the cloud computing provides a low-cost, scalable, and highly reliable infrastructure platform in the cloud.No up-front costs involved.\n",
    "\n",
    "In particular, why we use the following services:\n",
    "\n",
    "`S3`: Provides a relatively cheap, easy-to-use with scalability, high availability, security, and performance. \n",
    "\n",
    "`Spark`: This is simply the best framework for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "\n",
    "`EMR`: This is a cloud-native big data platform, allowing teams to process vast amounts of data quickly, and cost-effectively at scale using Spark. EMR is easy to use, secure, elastic and low-cost. Perfect to our project;\n",
    "\n",
    "#### `Propose how often the data should be updated and why`\n",
    "\n",
    "Since we receive one file per month it seems reasonable to update the model monthly.\n",
    "\n",
    "#### `Write a description of how you would approach the problem differently under the following scenarios`\n",
    "\n",
    "##### `The data was increased by 100x:`\n",
    "\n",
    "The biggest advantage of using spark on the cluster is to scale up and down. We can add more nodes to scale up\n",
    "\n",
    "##### `The data populates a dashboard that must be updated on a daily basis by 7am every day`\n",
    "The runnig interval of the Airflow DAG could be changed to daily and scheduled to run overnight to make the data available by 7am.\n",
    "\n",
    "##### `The database needed to be accessed by 100+ people.`\n",
    "\n",
    "Again cloud services like red shift are highly available. We can make use of elastic size feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
